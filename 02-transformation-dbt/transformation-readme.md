# ğŸ”„ Data Transformation Pipeline

## ğŸ“Œ Overview

This project handles the **transformation layer** of the BI/Data Engineering solution. It uses **dbt Core**, containerized with Docker, and orchestrates execution via **Azure Container Apps Jobs** and **Azure Data Factory**.

## ğŸ—ï¸ Architecture

**Flow:**
Databricks SQL Warehouse â‡† dbt Core (Docker) â Azure Container Registry â Azure Container Apps Job â Orchestrated by Prod ADF â Databricks SQL
Warehouse â Power BI

- **dbt Core**: Defines and runs SQL-based transformations.
- **Docker**: Local development and reproducibility.
- **Azure Container Registry (ACR)**: Stores dbt Docker images.
- **Azure Container Apps Jobs**: Runs containerized dbt transformations.
- **Azure Data Factory (ADF)**: Orchestrates job execution (POST + GET APIs).
- **Databricks SQL Warehouse (Unity Catalog)**: Transformation target.
- **Power BI**: Consumes curated datasets for reporting.

## ğŸ” Dev/Prod Setup & Security

- **Docker-based local dev** environment for dbt Core
- **GitHub Actions** for CI/CD to build & push dbt images into ACR
- **Azure Key Vault** for secret storage.
- **Managed Identity** for ACA â†” ADF â†” Databricks authentication.
- **Log Analytics** integrated with ACA for monitoring job runs.

## âš™ï¸ CI/CD (GitHub Actions)

- On code push, GitHub Actions build dbt Docker images.
- Images pushed to **Azure Container Registry (ACR)**.
- Deployed to **Azure Container Apps Jobs**.

## ğŸ“Š Monitoring

- **Log Analytics** workspace configured to track ACA job runs.
- Orchestration retries managed in ADF Until loop.

## ğŸ‘¤ Author

*Tunde Morakinyo*
