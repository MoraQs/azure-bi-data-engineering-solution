# ğŸš€ Data Ingestion Pipeline

## ğŸ“Œ Overview

This project handles the **ingestion layer** of the Azure BI/Data Engineering solution.

It extracts data from **Microsoft Fabric SQL Database**, loads it into **Azure Data Lake Storage (ADLS)**, and makes it available for downstream processing in **Azure Databricks Unity Catalog**.

## ğŸ—ï¸ Architecture

**Flow:**
Fabric SQL DB â Azure Data Factory (ADF) â ADLS (Parquet Partitioned) â
Databricks Autoloader â Unity Catalog (Bronze Layer)

- **Fabric SQL Database**: Source system.
- **Azure Data Factory (ADF)**: Orchestrates ingestion pipelines.
- **Azure Data Lake Storage (ADLS Gen2)**: Stores ingested data in **partitioned Parquet** format.
- **Databricks Autoloader**: Streams/micro-batches data into the **Bronze layer** of Unity Catalog.
- **Unity Catalog**: Provides governance and schema management.

## ğŸ” Dev/Prod Setup & Security

- **Resource Groups**: Separate Dev and Prod RGs.
- **ADF Instances**: Dev ADF + Prod ADF.
- **Storage Accounts**: Dev SA + Prod SA.
- **Authentication**: Managed Identities + Azure Key Vault for secrets.
- **Parameterization**: Pipelines parameterized to ensure environment reusability.
- **Triggers**: Scheduled & parameterized at the trigger level to avoid hardcoding credentials.

## âš™ï¸ CI/CD (Azure DevOps)

- Dev pipelines are integrated with **Git (Azure Repos)**.
- **Azure DevOps Pipelines** push code from Dev â Prod.
- **Parameter overrides** handled via YAML at deployment time.

## â° Scheduling

- Ingestion pipelines run on **scheduled triggers** (configurable).
- Supports both **batch** and **micro-batch** ingestion.

## ğŸ‘¤ Author

*Tunde Morakinyo*